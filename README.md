# BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical Applications

BodySLAM is a robust deep learning-based Simultaneous Localization and Mapping (SLAM) approach designed for endoscopic surgical applications. This framework effectively operates across various surgical settings, including laparoscopy, gastroscopy, and colonoscopy.

## Overview

BodySLAM addresses the challenges posed by hardware limitations and environmental variations in endoscopic surgeries. It integrates deep learning models with strong generalization capabilities to provide enhanced depth perception and 3D reconstruction capabilities.

The framework consists of three key modules:
1. Monocular Pose Estimation Module (MPEM)
2. Monocular Depth Estimation Module (MDEM)
3. 3D Reconstruction Module (3DM)

## Features

- Utilizes the state-of-the-art Zoe model for monocular depth estimation
- Implements a novel unsupervised pose estimation method, CyclePose, based on the CycleGAN architecture
- Provides robust performance across various endoscopic surgical settings
- Offers real-time capabilities with low inference time

## Coming Soon

- **Paper Publication**: A comprehensive paper detailing the BodySLAM framework, its methodology, and results is currently under review and will be available soon.

- **Code Refactoring**: We are working on refactoring the codebase to improve readability and ease of use. This update will make it easier for researchers and developers to understand and utilize the BodySLAM framework.

- **Improved Documentation**: A more detailed and user-friendly documentation is in the works. This will include installation instructions, usage guidelines, and examples to help users get started with BodySLAM.

## Installation

(Instructions for installation will be provided once the code refactoring is complete.)

## Usage

(Usage guidelines and examples will be added after the code refactoring and documentation improvements.)

## Citation

If you use BodySLAM in your research, please cite our upcoming paper:

(Citation details will be provided once the paper is published.)

## License

(License information will be added)


